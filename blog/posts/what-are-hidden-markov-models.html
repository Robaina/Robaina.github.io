<!DOCTYPE html>
<html translate="no" dir="ltr" lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1" name="viewport"/>
  <meta content="IE=edge" http-equiv="X-UA-Compatible"/>
  <meta content="In this post we introduce Hidden Markov Models and show Python implementations of two examples." name="description"/>
  <meta content="blog, hmm, hidden, markov, models, python" name="keywords"/>
  <meta content="Semidan Robaina Estevez" name="author"/>
  <title>
   Semid&aacute;n: What are Hidden Markov Models?
  </title>
  <link href="/imgs/my-logo.png" rel="icon" sizes="32x32" type="image/png"/>
  <link href="/css/master.css" rel="stylesheet"/>
  <link href="/css/blog.css" rel="stylesheet"/>
  <link href="/css/blog_entry.css" rel="stylesheet"/>
  <link href="/vendor/fontawesome_all.min.css" rel="stylesheet"/>
  <script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js">
  </script>
  <!-- highlight.js -->
  <!-- styles loaded dynamically in settings.js -->
  <script src="/vendor/highlight_js/highlight.pack.js">
  </script>
  <script src="/vendor/jquery-3.4.1.min.js">
  </script>
  <script>
   $(function() {
      $("#navbar-container").load("/navbar.html");
    });
    $(function() {
      $("#footer").load("/footer.html");
    });
    let logo = new Image();
    logo.src = "/imgs/my-logo.png";
  </script>
  <script src="/js/transitions.js">
  </script>
  <script src="/js/settings.js">
  </script>
  <script src="/js/blog-entry.js">
  </script>
 </head>
 <body>
  <div id="page-container">
   <div id="content-wrap">
    <div id="navbar-container">
    </div>
    <div id="blog-wrap">
     <div id="tag-container">
      <div class="topictag Statistics"></div>
      <div class="topictag Math"></div>
     </div>
     <div id="blog-entry-header">
      <p id="entry-title">
       What are Hidden Markov Models?
      </p>
      <p id="entry-date">
       01 May, 2019
      </p>
     </div>
     <article id="blog-content">
      <p>The first time I heard about Hidden Markov Models (HMMs) I was intrigued by their name. It definitely sounds intriguing. The combination of <em>hidden</em> — very mysterious by nature — and Markov, a Russian-sounding name got to me. Well, to be honest, I already knew about <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov processes</a>, but a hidden Markov, really, what's that?! Anyway, I immediately went to the web to learn a bit about HMMs and their applications. In doing so, I implemented two small examples of HMMs in Python — which I usually do whenever I'm learning about some new method. I'm sharing today these examples, perhaps they are useful to you! But first, let's introduce the general idea behind HMMs — there are plenty of online resources for a more in-depth study of HMMs and related topics.</p>
<p>In the simplest form, HMMs are formed by a random variable, $X$, which follows a Markov process. In a Markov process, the variable $X$ transitions from one value to another, or in the jargon, from one state to another, in a succession of time steps, and importantly, the probability of taking any given state depends solely on the state it had in the previous step — it only has short-term memory one could say. However, we can't observe the state that $X$ is in, hence the hidden part in HMMs. What we can observe, though, is the state of another random variable $Y$ which is <a href="https://en.wikipedia.org/wiki/Conditional_dependence">dependent</a> on $X$. Now, we typically know both, the conditional probabilities $P(X_n|X_{n-1})$ and $P(Y_n|X_n)$, being $n$ each time step, and the goal of a HMM is to infer the most likely sequence of hidden states $X_1, \dots, X_n$ given a sequence of values of the observable, dependent variable $Y_1,\dots,Y_n$.</p>
<p>Let's see a HMM in action in an example. I've taken it from the <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model">Wikipedia page on HMMs</a>:</p>
<img class="jupyter-post-image" alt="wikipedia_example" class="jupyter-post-image" src="/imgs/blog/what-are-hidden-markov-models/Example_HMM.png" style="max-height:500px;"/>
<p>In this toy example, $X$ represents the weather in the whereabouts of Bob's house and it has two possible states: Rainy or Sunny. On the other hand, the observable variable $Y$ has three values: Walk, Shop or Clean, and corresponds to Bob's actions in response to the weather conditions. Let each time step be one day, the above diagram depicts the <em>transition probability</em> between the two states. So for instance, if it was rainy yesterday, today there is a 70% chance it will rain again, whereas if yesterday was sunny there is a 40% chance it will rain today, and so on. Makes sense. Bob's response to weather is determined by the <em>emission probability</em> and it's also depicted in the diagram. For instance, Bob takes a walk on 10% of the rainy days while cleans the house on 50% of these days, clever guy.</p>
<p>So now the question is, given a sequence of actions across a time period, such as: Walk, Walk, Clean, Shop, Walk, what is the most likely sequence of weather conditions on the time period? And remember, we have no access to the weather forecast, we just know Bob's actions and the transition and emission probability for each pair of variable $X$ and $Y$ values. That's the kind of question that a HMM is meant to answer. In the following, we will use the Python library <a href="https://github.com/lopatovsky/HMMs">HMM</a> to find the most probable sequence of hidden states given the sequence of observations — behind the wheels, this library implements the <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm">Viterbi algorithm</a> to compute the most likely sequence.</p>
<pre><code class="python">import numpy as np
import pandas as pd
import hmms
from numpy.linalg import eig
from matplotlib import pyplot as plt
</code></pre>
<h2>Wikipedia example</h2>
<p>Here are the definitions of the states, $X$, and the observations $Y$, as well as the transition and emission probabilities depicted in the diagram above. Note that we must include an initial guess on the weather condition, i.e., for day one, for the Viterbi algorithm to work its way out of the problem. Most of the time, the initial guess is not very important since it gets quickly updated, we will see more on this later on.</p>
<pre><code class="python">states = ('Rainy', 'Sunny')

observations = ('walk', 'shop', 'clean')

start_probability = {'Rainy': 0.6, 'Sunny': 0.4}

transition_probability = pd.DataFrame({
   'Rainy' : {'Rainy': 0.7, 'Sunny': 0.3},
   'Sunny' : {'Rainy': 0.4, 'Sunny': 0.6},
   }, index=['Rainy', 'Sunny']).transpose()

emission_probability = pd.DataFrame({
   'Rainy' : {'walk': 0.1, 'shop': 0.4, 'clean': 0.5},
   'Sunny' : {'walk': 0.6, 'shop': 0.3, 'clean': 0.1},
   }, index=['walk', 'shop', 'clean']).transpose()
</code></pre>
<p>Let's transform these objects into the input required by the algorithm.</p>
<pre><code class="python">A = transition_probability.values
B = emission_probability.values
observed_variables = ['walk', 'walk', 'clean', 'shop', 'walk']
X = [observations.index(o) for o in observed_variables]
labels = [' Rainy', 'Sunny']

# Implementing the problem in the hmms library
Pi= np.array(list(start_probability.values()))
dhmm = hmms.DtHMM(A,B,Pi)
e_seq = np.array(X)
(log_prob, s_seq) =  dhmm.viterbi(e_seq)

# Let's print the most likely state sequence
plt.rcParams['figure.figsize'] = [10,5]
state_labels = {0: 'S', 1: 'R'}
observation_labels = {0: 'W', 1: 'C', 2: 'Sh'}
hmms.plot_hmm(s_seq, e_seq, state_labels=state_labels,
              observation_labels=observation_labels)
print(f'Probability: {np.exp(log_prob):.5f}')
</code></pre>
<img class="jupyter-post-image" alt="png" class="jupyter-post-image" src="/imgs/blog/what-are-hidden-markov-models/output_5_0.png" style="max-height:400px; max-width:600px;"/>
<pre><code>Log(Probability) = -3.0605
</code></pre>
<p>So there we go. The most likely sequence of weather states, as found out by the Viterbi algorithm, is: Rainy, Rainy, Sunny, Sunny and Rainy. Note that the Viterbi algorithm also outputs a probability, this is the probability of the sequence of states given the sequence of observables that we just fed to the model, and it is computed multiplying all transition and emission probabilities out.</p>
<h2>Classifying a gene sequence</h2>
<p>This toy example is taken from this <a href="https://www.nature.com/articles/nbt1004-1315">paper</a> [1] — a good non-mathy introduction to HMMs — where the sequence of observables is a nucleotide sequence, with labels equal to the nucleobases: $A, C, G, T$, and the hidden states are the labels <em>exon</em>, <em>5'</em> and <em>intron</em>, which correspond to different functional parts of a gene.</p>
<img class="jupyter-post-image" alt="hmm_model" class="jupyter-post-image" src="/imgs/blog/what-are-hidden-markov-models/Gene_HMM.png"/>
<p>Hence, we are predicting the most likely sequence of gene labels given the observed nucleotide sequence and the Markov model provided above. Note that each gene region has emission probabilities for each nucleobase: exons have a uniform distribution of base types, while introns are enriched in $A$ and $T$, and the 5' region is composed almost entirely of $G$. We are going to modify slightly the model presented in the paper. Specifically, we won't consider the begin and end states, since they don't have emission probabilities. Consequently, the probability of transiting from intron to intron is changed to 1. Let's go with the Python implementation.</p>
<pre><code class="python">states = ('E', '5', 'I')
observations = ('A', 'C', 'G', 'T')

transition_probability = pd.DataFrame({
    'E' : {'E' : 0.9, '5' : 0.1, 'I' : 0},
    '5' : {'E' : 0, '5' : 0, 'I' : 1},
    'I' : {'E': 0, '5' : 0, 'I' : 1}
    }, index=['E','5','I']).transpose()


emission_probability = pd.DataFrame({
   'E' : {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T' : 0.25},
   '5' : {'A': 0.05, 'C': 0, 'G': 0.95, 'T' : 0},
   'I' : {'A': 0.4, 'C': 0.1, 'G': 0.1, 'T' : 0.4}
   }, index=['A','C','G','T']).transpose()


observed_variables = ['C','T','T','C','A','T','G','T','G','A','A','A','G',
                      'C','A','G','A','C','G','T','A','A','G','T','C','A']

X = [observations.index(o) for o in observed_variables]

# Implementing the problem in the hmms library
A = transition_probability.values
B = emission_probability.values
Pi = np.array([1,0,0])
dhmm = hmms.DtHMM(A,B,Pi)

# Finding most likely sequence of labels
e_seq = np.array(X)
( log_prob, s_seq ) =  dhmm.viterbi( e_seq )

plt.rcParams['figure.figsize'] = [20,20]
state_labels = dict(zip(range(3),states))
observation_labels = dict(zip(range(4),observations))
hmms.plot_hmm(s_seq, e_seq, state_labels=state_labels,
              observation_labels=observation_labels)
print('Log(Probability): ' + str(log_prob))
</code></pre>
<p><img class="jupyter-post-image" alt="png" src="/imgs/blog/what-are-hidden-markov-models/output_8_0.png"/></p>
<pre><code>Log(Probability): -38.284929499081535
</code></pre>
<p>And there we have it, the most likely succession of states according to the Viterbi algorithm. The majority of the sequence is labeled as an exon, which after a sharp transition gives rise to the last seven nucleobases, labeled as intron.</p>
<h2>Concluding remarks</h2>
<p>Ok, it has been a very shallow introduction to HMMs, but I hope at least I caught your attention and pushed you to learn more about them. HMMs have many applications, including Bioinformatics where more elaborated models than the toy example presented here are employed to classify nucleotide sequences. HMMs can also be employed in reverse, to find out missing transition or emission probabilities that best explain a collection of sequences of states and observations. Lastly, if you know a bit about Bayesian statistics you probably have noticed the connections between HMMs and <a href="https://en.wikipedia.org/wiki/Bayesian_network">Bayesian networks</a> and other sorts of Bayesian inference. Perhaps I'll write about this topic another time since Bayesian networks are very cool! But that's all for now...</p>
<h2>References</h2>
<p>[1] Eddy, S. What is a hidden Markov model?. Nat Biotechnol 22, 1315–1316 (2004). https://doi.org/10.1038/nbt1004-1315</p>
     </article>
    </div>
    <div id="blog-navigator">
    </div>
    <div id="suggested-readings">
     <h1 id="suggested-readings-title">
      Related posts:
     </h1>
     <div id="suggested-readings-grid">
     </div>
    </div>
    <!-- Blog comments (commentbox.io) -->
    <div class="commentbox"></div>
   </div>
   <footer id="footer">
   </footer>
  </div>
  <script src="/js/scrollToTop.js">
  </script>
  <div id="return-to-start-arrow" onclick="scrollToTop()">
  </div>
  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        },
        "HTML-CSS": {
          linebreaks: { automatic: true, width: "container" }
        }
      });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=default">
  </script>
 </body>
</html>
